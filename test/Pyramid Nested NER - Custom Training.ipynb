{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JFx0z_eJmowS"
   },
   "source": [
    "# Pyramid: A Layered Model for Nested Named Entity Recognition - Custom Training\n",
    "\n",
    "This notebook provides a blueprint to train a version of [Pyramid: A Layered Model for Nested Named Entity Recognition](https://www.aclweb.org/anthology/2020.acl-main.525.pdf) on your own data.\n",
    "\n",
    "**It is recommended to use GPU for training.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "csZXCEfTmXT5"
   },
   "source": [
    "## Downloading Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hf4lfY0_aa4E"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/federico-giannoni/pyramid-nested-ner.git\n",
    "!mv pyramid-nested-ner/* . && rm -rf pyramid-nested-ner # move to root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4KgL_0eVdAaq"
   },
   "outputs": [],
   "source": [
    "!pip install flair seqeval 2>&1 > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xFEHF296Z2pB"
   },
   "outputs": [],
   "source": [
    "from pyramid_nested_ner.model import PyramidNer\n",
    "from pyramid_nested_ner.data import DataPoint, Entity\n",
    "from pyramid_nested_ner.modules.word_embeddings.transformer_embeddings import TransformerWordEmbeddings\n",
    "from pyramid_nested_ner.modules.word_embeddings.pretrained_embeddings import PretrainedWordEmbeddings\n",
    "from pyramid_nested_ner.data.dataset import PyramidNerDataset\n",
    "from pyramid_nested_ner.utils.data import rasa_data_reader\n",
    "from pyramid_nested_ner.training.trainer import PyramidNerTrainer\n",
    "from pyramid_nested_ner.training.optim import get_default_sgd_optim\n",
    "from copy import deepcopy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5iF3VdA1kI2N"
   },
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b02zkumvpfZK"
   },
   "source": [
    "## Experiments Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C529p7A5jPW0"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gbr4Cy0gCuZH"
   },
   "outputs": [],
   "source": [
    "# TODO: set these paths to point to your data\n",
    "\n",
    "train_data_path = None  # e.g. \"/content/drive/My Drive/your_train_data_path\"\n",
    "test_data_path = None   # e.g. \"/content/drive/My Drive/your_test_data_path\"\n",
    "dev_data_path  = None   # e.g \"/content/drive/My Drive/your_dev_data_path\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uFv4Us9yJZNf"
   },
   "outputs": [],
   "source": [
    "def your_own_data_generator(path):\n",
    "  \"\"\"\n",
    "  Implement this function to yield DataPoint objects\n",
    "  representing your data. The class definition can be found at: \n",
    "  https://github.com/federico-giannoni/pyramid-nested-ner/blob/main/pyramid_nested_ner/data/__init__.py\n",
    "  \"\"\"\n",
    "  # if your data is in rasa json format, you can just uncomment the line below:\n",
    "  # yield from rasa_data_reader(path)\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RMbGmbODEBQI"
   },
   "outputs": [],
   "source": [
    "pyramid_max_depth = 2  # keep this low if you plan to do inference on CPU (paper uses 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iz6-csJCjye_"
   },
   "outputs": [],
   "source": [
    "train_data = PyramidNerDataset(\n",
    "  your_own_data_generator(train_data_path), \n",
    "  pyramid_max_depth=pyramid_max_depth,\n",
    "  token_lexicon=lexicon,\n",
    "  custom_tokenizer=None, \n",
    "  char_vectorizer=True,\n",
    ").get_dataloader(\n",
    "    shuffle=True,\n",
    "    batch_size=64,\n",
    "    device=DEVICE, \n",
    "    bucketing=True\n",
    ")\n",
    "\n",
    "test_data = PyramidNerDataset(\n",
    "  your_own_data_generator(test_data_path), \n",
    "  pyramid_max_depth=pyramid_max_depth,\n",
    "  token_lexicon=lexicon,\n",
    "  custom_tokenizer=None, \n",
    "  char_vectorizer=True,\n",
    ").get_dataloader(\n",
    "    shuffle=True, \n",
    "    batch_size=16,\n",
    "    device=DEVICE, \n",
    "    bucketing=True\n",
    ")\n",
    "\n",
    "dev_data = PyramidNerDataset(\n",
    "  your_own_data_generator(dev_data_path), \n",
    "  pyramid_max_depth=pyramid_max_depth,\n",
    "  token_lexicon=lexicon,\n",
    "  custom_tokenizer=None, \n",
    "  char_vectorizer=True,\n",
    ").get_dataloader(\n",
    "    shuffle=True, \n",
    "    batch_size=16,\n",
    "    device=DEVICE, \n",
    "    bucketing=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "960EPJjtqFLm"
   },
   "source": [
    "## Training\n",
    "\n",
    "For the `language_model` parameter, you can use any huggingface transformer by specyfing its name. The full list of names is available [here](https://huggingface.co/transformers/pretrained_models.html).\n",
    "\n",
    "If you plan on using an uncased model, you should also pass `language_model_casing=False` to the `PyramidNer` constructor.\n",
    "\n",
    "**Note that using word embeddings from pre-trained language models increases training time (and inference time) by a factor of 10.** During training, embeddings are cached during the first epoch so that the following epochs are faster, but this can not be done during inference. For this reason, you should only use the `language_model` parameter if you're planning on using the model for research purposes.\n",
    "\n",
    "For the `word_embeddings` parameter, you can either provide your own `torch.nn.Embedding` module, or use the names of any of the `WordEmbeddings` from **Flair** that you can find [here](https://github.com/flairNLP/flair/blob/master/flair/embeddings/token.py#L121)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5O7Hxp6fkVM1"
   },
   "outputs": [],
   "source": [
    "pyramid_ner = PyramidNer(\n",
    "  word_lexicon=lexicon,\n",
    "  entities_lexicon=train_entities,\n",
    "  word_embeddings=['en-glove', 'en-crawl'],  # 100-dim glove + fasttext\n",
    "  language_model=None,\n",
    "  char_embeddings_dim=60,\n",
    "  encoder_hidden_size=100,\n",
    "  encoder_output_size=200,\n",
    "  decoder_hidden_size=100,\n",
    "  inverse_pyramid=False,\n",
    "  custom_tokenizer=None,\n",
    "  pyramid_max_depth=pyramid_max_depth,\n",
    "  decoder_dropout=0.2,\n",
    "  encoder_dropout=0.2,\n",
    "  device=DEVICE,\n",
    ")\n",
    "\n",
    "trainer = PyramidNerTrainer(pyramid_ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "US7pzs39IISA"
   },
   "outputs": [],
   "source": [
    "# default optimizer and LR scheduler as described in the paper - feel free to change them.\n",
    "optimizer, scheduler = get_default_sgd_optim(pyramid_ner.nnet.parameters()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pB8gqNIHl7BB"
   },
   "outputs": [],
   "source": [
    "ner_model, report = trainer.train(\n",
    "  train_data, \n",
    "  optimizer=optimizer, \n",
    "  scheduler=scheduler, \n",
    "  restore_weights_on='loss',\n",
    "  epochs=60, \n",
    "  dev_data=dev_data, \n",
    "  patience=np.inf, \n",
    "  grad_clip=5.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yy7enskYaMW6"
   },
   "outputs": [],
   "source": [
    "report.plot_loss_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pAqJYkPh7Yz1"
   },
   "outputs": [],
   "source": [
    "report.plot_custom_report('micro_f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a07YH0nH1-el"
   },
   "outputs": [],
   "source": [
    "print(trainer.test_model(test_data, out_dict=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AeQqiUwOqtOx"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JpWQSbetqtO3"
   },
   "outputs": [],
   "source": [
    "out = pyramid_ner.parse(\"your own test sentence\")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tkw3iEy3IiwN"
   },
   "source": [
    "## Saving for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lP7pM82JqtO9"
   },
   "outputs": [],
   "source": [
    "pyramid_ner.save(path='.', name='pyramid_ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jujiA_1KqtO_"
   },
   "outputs": [],
   "source": [
    "!tar -cvzf pyramid_ner  # compress..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VJLcsURfI6Nz"
   },
   "source": [
    "You can load it back using:\n",
    "\n",
    "```python\n",
    "from pyramid_nested_ner.model import PyramidNer\n",
    "pyramid_ner = PyramidNer.load(path, custom_tokenizer=None, force_device=None, force_language_model=None, force_embeddings=None)\n",
    "```\n",
    "\n",
    "Where `force_device`, `force_language_model` and `force_embeddings` allow you to overwrite the `device`, `language_model` and `word_embeddings` parameters that were provided when the model was saved."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Pyramid Nested NER: Custom Training",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
